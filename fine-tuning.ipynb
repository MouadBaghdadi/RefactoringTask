{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9110656,"sourceType":"datasetVersion","datasetId":5498826},{"sourceId":89395,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":74994,"modelId":99718}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import random\nimport os\nimport pickle\nimport time\nimport datetime\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import StepLR\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport re","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:52:04.073193Z","iopub.execute_input":"2024-08-05T16:52:04.073855Z","iopub.status.idle":"2024-08-05T16:52:04.079434Z","shell.execute_reply.started":"2024-08-05T16:52:04.073823Z","shell.execute_reply":"2024-08-05T16:52:04.078363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install torchtriton --extra-index-url \"https://download.pytorch.org/whl/nightly/cu117\"","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:52:04.081039Z","iopub.execute_input":"2024-08-05T16:52:04.081343Z","iopub.status.idle":"2024-08-05T16:52:04.091894Z","shell.execute_reply.started":"2024-08-05T16:52:04.081320Z","shell.execute_reply":"2024-08-05T16:52:04.090994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the random seed for reproducibility\nseed = 42\ntorch.manual_seed(seed)\nrandom.seed(seed)\nnp.random.seed(seed)\n\n# Set the device to GPU if available, otherwise CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device set to {device}.\")","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:52:04.094415Z","iopub.execute_input":"2024-08-05T16:52:04.095040Z","iopub.status.idle":"2024-08-05T16:52:04.103008Z","shell.execute_reply.started":"2024-08-05T16:52:04.095014Z","shell.execute_reply":"2024-08-05T16:52:04.102083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_DIR=\"./../input/fine-tuning-data\"\n","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:52:04.104880Z","iopub.execute_input":"2024-08-05T16:52:04.105212Z","iopub.status.idle":"2024-08-05T16:52:04.115325Z","shell.execute_reply.started":"2024-08-05T16:52:04.105189Z","shell.execute_reply":"2024-08-05T16:52:04.114469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Attempt to derive vocab_size from the dataset\n\nmeta_path = os.path.join(DATA_DIR, 'meta.pkl')\nvocab_size = None\n\nif os.path.exists(meta_path):\n    with open(meta_path, 'rb') as f:\n        meta = pickle.load(f)\n    vocab_size = meta['vocab_size']\n    print(f\"found vocab_size = {vocab_size} (inside {meta_path})\")\nelse:\n    print(\"Meta file not found. Please ensure the meta.pkl file is present in the data directory.\")\n\n# Encode and decode functions for character-level Tokenzation \ndef encode(s):\n    return [meta['stoi'][c] for c in s]\n\ndef decode(l):\n    return ''.join([meta['itos'][i] for i in l])","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:52:04.116450Z","iopub.execute_input":"2024-08-05T16:52:04.116750Z","iopub.status.idle":"2024-08-05T16:52:04.130211Z","shell.execute_reply.started":"2024-08-05T16:52:04.116728Z","shell.execute_reply":"2024-08-05T16:52:04.129412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load encoded data\ntrain_data = np.memmap(os.path.join(DATA_DIR, 'train.bin'), dtype=np.uint16, mode='r')\nval_data = np.memmap(os.path.join(DATA_DIR, 'val.bin'), dtype=np.uint16, mode='r')\nmodel_path = \"/kaggle/input/tiny-llm-10m/pytorch/default/1/10M_2024-07-21_08-16.pth\"","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:52:04.131164Z","iopub.execute_input":"2024-08-05T16:52:04.132899Z","iopub.status.idle":"2024-08-05T16:52:04.139189Z","shell.execute_reply.started":"2024-08-05T16:52:04.132875Z","shell.execute_reply":"2024-08-05T16:52:04.138470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hyperparameters for the GPT model\nblock_size = 256  # Maximum context length\nvocab_size=53\nn_embd = 372      # Embedding dimension\nn_head = 6        # Number of attention heads\nn_layer = 6       # Number of transformer blocks\ndropout = 0       # Dropout rate\nbatch_size = 64   # Batch size for training\nmax_iters = 10000  # Maximum number of iterations\nlearning_rate = 1e-3 # Initial Learning rate value\nmiles = [int(max_iters * m) for m in [0.7, 0.8, 0.9]]  # Milestones for learning rate decay as fractions of max_iters\neval_interval = 1000 # Evaluation interval\neval_iters = 500  # Number of iterations for evaluation\n\ncompile = False # requires PyTorch 2.0\n\n# Rank value for lora\n\nr=8","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:52:04.141049Z","iopub.execute_input":"2024-08-05T16:52:04.141642Z","iopub.status.idle":"2024-08-05T16:52:04.147657Z","shell.execute_reply.started":"2024-08-05T16:52:04.141611Z","shell.execute_reply":"2024-08-05T16:52:04.146899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LayerNorm(nn.Module):\n    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input):\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n\nclass Head(nn.Module):\n    \"\"\"One head of self-attention.\"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x)   # (B, T, 16)\n        q = self.query(x) # (B, T, 16)\n        v = self.value(x)\n        \n        out = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=dropout if self.training else 0, is_causal=True)\n            \n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"multiple heads of self-attention in parallel.\"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n    \nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity.\"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd, bias=False),\n            nn.GELU(),\n            nn.Linear( 4 * n_embd, n_embd, bias=False),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n    \nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by feedforward.\"\"\"\n\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedForward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd, bias=False)\n        self.ln2 = nn.LayerNorm(n_embd, bias=False)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\nclass GPT(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd, bias=False) \n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C)\n        x = self.ln_f(x) # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:] # (B, T)\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:52:04.207628Z","iopub.execute_input":"2024-08-05T16:52:04.207908Z","iopub.status.idle":"2024-08-05T16:52:04.233502Z","shell.execute_reply.started":"2024-08-05T16:52:04.207885Z","shell.execute_reply":"2024-08-05T16:52:04.232563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nclass LoraHead(Head):\n    \"\"\"\n    Extends MultiHeadAttention with LoRA (Low-Rank Adaptation) matrices.\n    LoRA enhances efficiency by only updating the query and value matrices.\n    This class adds LoRA matrices and applies LoRA logic in the forward method.\n\n    Parameters:\n    - r (int): Rank for LoRA matrices.\n    - config: Configuration of the Roberta Model.\n    \"\"\"\n    \n    def __init__(self, r=8):\n        head_size = n_embd // n_head\n        super().__init__(head_size=head_size)\n        \n        self.lora_query_matrix_A = nn.Parameter(torch.zeros(r, head_size))\n        self.lora_query_matrix_B = nn.Parameter(torch.randn(n_embd, r))\n        self.lora_value_matrix_A = nn.Parameter(torch.zeros(r, head_size))\n        self.lora_value_matrix_B = nn.Parameter(torch.randn(n_embd, r))\n        self.lora_key_matrix_A = nn.Parameter(torch.zeros(r, head_size))\n        self.lora_key_matrix_B = nn.Parameter(torch.randn(n_embd, r))\n\n    def lora_query(self, x):\n        \"\"\"\n        Applies LoRA to the query component. Computes a modified query output by adding \n        the LoRA adaptation to the standard query output. Requires the regular linear layer \n        to be frozen before training.\n        \"\"\"\n        lora_query_weights = torch.matmul(self.lora_query_matrix_B, self.lora_query_matrix_A)\n\n        return self.query(x) + F.linear(x, lora_query_weights.T)\n\n    def lora_value(self, x):\n        \"\"\"\n        Applies LoRA to the value component. Computes a modified value output by adding \n        the LoRA adaptation to the standard value output. Requires the regular linear layer \n        to be frozen before training.\n        \"\"\"\n        lora_value_weights = torch.matmul(self.lora_value_matrix_B, self.lora_value_matrix_A)\n\n\n        return self.value(x) +  F.linear(x, lora_value_weights.T)\n\n    def lora_key(self, x):\n        \"\"\"\n        Applies LoRA to the key component. Computes a modified value output by adding \n        the LoRA adaptation to the standard value output. Requires the regular linear layer \n        to be frozen before training.\n        \"\"\"\n        lora_key_weights = torch.matmul(self.lora_key_matrix_B, self.lora_key_matrix_A)\n\n        return self.key(x) +  F.linear(x, lora_key_weights.T)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.lora_key(x)\n        q = self.lora_query(x)\n        v = self.lora_value(x)\n\n        out = torch.nn.functional.scaled_dot_product_attention(\n            q, k, v, attn_mask=None, dropout_p=dropout if self.training else 0, is_causal=True\n        )\n\n        return out\nclass LoraGPT(nn.Module):\n    \n    \n    def __load_model(self)->GPT:\n        \n\n\n        model = GPT()\n        #try:\n        #    model = torch.compile(model)  # requires PyTorch 2.0\n        #except Exception as e:\n        #    pass\n        model.load_state_dict(torch.load(model_path),strict=False)\n\n        \n        return model\n    \n    def __init__(self,  r=8):\n        \n        \n        super().__init__()\n        self.lora_rank = r\n  \n        self.model=self.__load_model()\n        self.replace_multihead_attention_recursion(self.model)\n        self.freeze_parameters_except_lora_and_bias()\n        \n        \n    def forward(self, x,targets):\n        return self.model(x,targets)\n    def generate(self, idx, max_new_tokens):\n        return self.model.generate(idx, max_new_tokens)\n   \n        \n    def replace_multihead_attention_recursion(self,model):\n        \"\"\"\n        Replaces RobertaSelfAttention with LoraRobertaSelfAttention in the model.\n        This method applies the replacement recursively to all sub-components.\n\n        Parameters\n        ----------\n        model : nn.Module\n            The PyTorch module or model to be modified.\n        \"\"\"\n        for name, module in model.named_children():\n            if isinstance(module, Head):\n                # Replace RobertaSelfAttention with LoraRobertaSelfAttention\n                new_layer = LoraHead(r=self.lora_rank)\n                new_layer.load_state_dict(module.state_dict(), strict=False)\n                setattr(model, name, new_layer)\n            else:\n                # Recursive call for child modules\n                self.replace_multihead_attention_recursion(module)\n                \n                \n    def freeze_parameters_except_lora_and_bias(self):\n        \"\"\"\n        Freezes all model parameters except for specific layers and types based on the configuration.\n        Parameters in LoRA layers, the finetune head, bias parameters, embeddings, and layer norms \n        can be set as trainable based on class settings.\n        \"\"\"\n        for name, param in self.model.named_parameters():\n\n            is_trainable = (\n                \"lora_\" in name \n                \n                #(self.train_layer_norms and \"LayerNorm\" in name)\n            )\n            param.requires_grad = is_trainable\n        ","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:52:04.291493Z","iopub.execute_input":"2024-08-05T16:52:04.291822Z","iopub.status.idle":"2024-08-05T16:52:04.311883Z","shell.execute_reply.started":"2024-08-05T16:52:04.291787Z","shell.execute_reply":"2024-08-05T16:52:04.310983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n#model=GPT().to(device)\nmodel = LoraGPT(r=r)\nm=model.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:52:04.313281Z","iopub.execute_input":"2024-08-05T16:52:04.313535Z","iopub.status.idle":"2024-08-05T16:52:04.576278Z","shell.execute_reply.started":"2024-08-05T16:52:04.313514Z","shell.execute_reply":"2024-08-05T16:52:04.575249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\nnum_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(num_parameters)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:52:04.577376Z","iopub.execute_input":"2024-08-05T16:52:04.577658Z","iopub.status.idle":"2024-08-05T16:52:04.584128Z","shell.execute_reply.started":"2024-08-05T16:52:04.577628Z","shell.execute_reply":"2024-08-05T16:52:04.583341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get random batch of data\ndef get_batch(split):\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n# Estimate loss on train and val splits\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters) \n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:52:04.586205Z","iopub.execute_input":"2024-08-05T16:52:04.586501Z","iopub.status.idle":"2024-08-05T16:52:04.595843Z","shell.execute_reply.started":"2024-08-05T16:52:04.586479Z","shell.execute_reply":"2024-08-05T16:52:04.594982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\n# Initialize learning rate scheduler\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=miles, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:52:04.596896Z","iopub.execute_input":"2024-08-05T16:52:04.597211Z","iopub.status.idle":"2024-08-05T16:52:04.611560Z","shell.execute_reply.started":"2024-08-05T16:52:04.597181Z","shell.execute_reply":"2024-08-05T16:52:04.610821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get current date and hour to get track of experiments\nnow = datetime.datetime.now()\ndate_hour = now.strftime(\"%Y-%m-%d_%H-%M\")\n\n# Train\n# Start training timer\nstart_time = time.time()\n\n# Training loop\nfor iter in range(max_iters):\n\n    # evaluate the model on the train and val splits and log the losses\n    if iter % eval_interval == 0:\n        print(\"loss estimation\")\n        losses = estimate_loss()\n        print(f'iter {iter:5d} | train loss {losses[\"train\"]:.4f} | val loss {losses[\"val\"]:.4f}')\n        \n    # train the model for one iteration\n    xb, yb = get_batch('train')\n\n    # forward pass\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n    # Step the scheduler\n    \n    \n    \n    \n    scheduler.step()\n    if iter%50==0:\n        print('passed 50 iteration')\n\n# End training timer\nend_time = time.time()\nprint(f'Training time: {(end_time - start_time) / 60}  min')\n\n# Save the trained model\ntorch.save(model.state_dict(), f\"{num_parameters}_{date_hour}.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-08-05T16:52:04.612800Z","iopub.execute_input":"2024-08-05T16:52:04.613107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = np.memmap(os.path.join(DATA_DIR, 'test.bin'), dtype=np.uint16, mode='r')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_example(example, model, max_new_tokens=30):\n\n    # Split example and determine maximum new tokens allowed\n    splited_example = example.split(\"# reformulation\")\n    if not (\"for\" in splited_example[0]):\n        max_new_tokens = 22\n    # Encode prompt and prepare for evaluation\n    encoded_example = torch.tensor(encode(splited_example[0] + \"# reformulation\"), dtype=torch.long).unsqueeze(0).to(device)\n    prompt_text = splited_example[0] + \"# reformulation\"\n\n    result_example = splited_example[-1]\n\n    #print(\"result: ==>\",result_example)\n\n    # Extract real results from example\n    #real_results = [float(match.group()) for match in re.finditer(r\"(?<=# )-?\\d+(\\.\\d+)?\", result_example.split('\\n\\n')[0].replace(\"\\n\", \"\"))]\n\n    # Generate response from model and extract generated results\n    response = decode(model.generate(encoded_example, max_new_tokens=max_new_tokens)[0].tolist())\n    splited_response = response.split(\"# reformulation\")\n    result_response = splited_response[-1]\n    #generated_results = [float(match.group()) for match in re.finditer(r\"(?<=# )-?\\d+(\\.\\d+)?\", result_response.split('\\n\\n')[0].replace(\"\\n\", \"\"))]\n\n    return prompt_text, result_example, result_response\n\n\n\n# Write results to file\ndef write_results_to_file(output_file, prompt, real_results, generated_results):\n    df = pd.DataFrame({\n        'Prompt': prompt,\n        'Real_Results': real_results,\n        'Generated_Results': generated_results\n    })\n    df.to_csv(output_file, index=False)\n\n\n\ndef evaluate_pair(real, generated_result):\n    # Determine the length of the shorter and longer strings\n    min_len = min(len(real), len(generated_result))\n    max_len = max(len(real), len(generated_result))\n\n    # Count the number of matching characters at the same index\n    match_count = sum(1 for i in range(min_len) if real[i] == generated_result[i])\n\n    # Calculate the ratio of matches to the length of the longer string\n    ratio = match_count / max_len\n    return ratio\n\n# Evaluation Loop\n\n# Split examples and initialize lists for results\nexamples = decode(test_data).split(\"\\n\\n\")\nexamples = [example for example in examples if example]\n\n# Start evaluation process\nprompt = []\nreal_results = []\ngenerated_results = []\n\n# Iterate through examples and evaluate the model on each one\nfor example in tqdm(examples):\n    prompt_text, real_result, result = evaluate_example(example, model)\n    prompt.append(prompt_text)\n    real_results.append(real_result)\n    generated_results.append(result)\n\n# Calculate and print accuracy\nscore=0\n\nfor real,generated in zip(real_results, generated_results):\n  score+=evaluate_pair(real,generated)\naccuracy = score / len(generated_results)\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")\n\n# Store accuracy in a file\nwith open(\"accuracy.txt\", 'w') as f:\n    f.write(f\"Accuracy: {accuracy * 100:.2f}%\\n\")\n\n# Store predictions in a CSV file\n    write_results_to_file(\"predictions.csv\", prompt, real_results, generated_results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}