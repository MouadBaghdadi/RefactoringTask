{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to cpu.\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Set the device to GPU if available, otherwise CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device set to {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to load and save data\n",
    "def save_data(data, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(data)\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR=\"./\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found vocab_size = 36 (inside ./meta.pkl)\n"
     ]
    }
   ],
   "source": [
    "# Attempt to derive vocab_size from the dataset\n",
    "\n",
    "meta_path = os.path.join(DATA_DIR, 'meta.pkl')\n",
    "vocab_size = None\n",
    "\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    vocab_size = meta['vocab_size']\n",
    "    print(f\"found vocab_size = {vocab_size} (inside {meta_path})\")\n",
    "else:\n",
    "    print(\"Meta file not found. Please ensure the meta.pkl file is present in the data directory.\")\n",
    "\n",
    "# Encode and decode functions for character-level Tokenzation \n",
    "def encode(s):\n",
    "    return [meta['stoi'][c] for c in s]\n",
    "\n",
    "def decode(l):\n",
    "    return ''.join([meta['itos'][i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded data saved as binary files.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_data = load_data(os.path.join(DATA_DIR, 'train.txt'))\n",
    "val_data = load_data(os.path.join(DATA_DIR, 'val.txt'))\n",
    "test_data = load_data(os.path.join(DATA_DIR, 'test.txt'))\n",
    "\n",
    "# Encode data\n",
    "train_ids = encode(train_data)\n",
    "val_ids = encode(val_data)\n",
    "test_ids = encode(test_data)\n",
    "\n",
    "# Save encoded data to bin files, make sure to choose \"Files only\" on the persistence option of the session so that you don't encode data each time\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "test_ids = np.array(test_ids, dtype=np.uint16)\n",
    "\n",
    "train_ids.tofile( 'train.bin')\n",
    "val_ids.tofile( 'val.bin')\n",
    "test_ids.tofile('test.bin')\n",
    "\n",
    "print(\"Encoded data saved as binary files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(train_ids)\n",
    "del(val_ids)\n",
    "del(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load encoded data\n",
    "train_data = np.memmap(\"./train.bin\", dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(\"./val.bin\", dtype=np.uint16, mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the model...\n",
      "\n",
      "_orig_mod.token_embedding_table.weight\n",
      "_orig_mod.position_embedding_table.weight\n",
      "_orig_mod.blocks.0.sa.heads.0.lora_query_matrix_B\n",
      "_orig_mod.blocks.0.sa.heads.0.lora_query_matrix_A\n",
      "_orig_mod.blocks.0.sa.heads.0.lora_value_matrix_B\n",
      "_orig_mod.blocks.0.sa.heads.0.lora_value_matrix_A\n",
      "_orig_mod.blocks.0.sa.heads.0.key.weight\n",
      "_orig_mod.blocks.0.sa.heads.0.query.weight\n",
      "_orig_mod.blocks.0.sa.heads.0.value.weight\n",
      "_orig_mod.blocks.0.sa.heads.1.lora_query_matrix_B\n",
      "_orig_mod.blocks.0.sa.heads.1.lora_query_matrix_A\n",
      "_orig_mod.blocks.0.sa.heads.1.lora_value_matrix_B\n",
      "_orig_mod.blocks.0.sa.heads.1.lora_value_matrix_A\n",
      "_orig_mod.blocks.0.sa.heads.1.key.weight\n",
      "_orig_mod.blocks.0.sa.heads.1.query.weight\n",
      "_orig_mod.blocks.0.sa.heads.1.value.weight\n",
      "_orig_mod.blocks.0.sa.heads.2.lora_query_matrix_B\n",
      "_orig_mod.blocks.0.sa.heads.2.lora_query_matrix_A\n",
      "_orig_mod.blocks.0.sa.heads.2.lora_value_matrix_B\n",
      "_orig_mod.blocks.0.sa.heads.2.lora_value_matrix_A\n",
      "_orig_mod.blocks.0.sa.heads.2.key.weight\n",
      "_orig_mod.blocks.0.sa.heads.2.query.weight\n",
      "_orig_mod.blocks.0.sa.heads.2.value.weight\n",
      "_orig_mod.blocks.0.sa.heads.3.lora_query_matrix_B\n",
      "_orig_mod.blocks.0.sa.heads.3.lora_query_matrix_A\n",
      "_orig_mod.blocks.0.sa.heads.3.lora_value_matrix_B\n",
      "_orig_mod.blocks.0.sa.heads.3.lora_value_matrix_A\n",
      "_orig_mod.blocks.0.sa.heads.3.key.weight\n",
      "_orig_mod.blocks.0.sa.heads.3.query.weight\n",
      "_orig_mod.blocks.0.sa.heads.3.value.weight\n",
      "_orig_mod.blocks.0.sa.heads.4.lora_query_matrix_B\n",
      "_orig_mod.blocks.0.sa.heads.4.lora_query_matrix_A\n",
      "_orig_mod.blocks.0.sa.heads.4.lora_value_matrix_B\n",
      "_orig_mod.blocks.0.sa.heads.4.lora_value_matrix_A\n",
      "_orig_mod.blocks.0.sa.heads.4.key.weight\n",
      "_orig_mod.blocks.0.sa.heads.4.query.weight\n",
      "_orig_mod.blocks.0.sa.heads.4.value.weight\n",
      "_orig_mod.blocks.0.sa.heads.5.lora_query_matrix_B\n",
      "_orig_mod.blocks.0.sa.heads.5.lora_query_matrix_A\n",
      "_orig_mod.blocks.0.sa.heads.5.lora_value_matrix_B\n",
      "_orig_mod.blocks.0.sa.heads.5.lora_value_matrix_A\n",
      "_orig_mod.blocks.0.sa.heads.5.key.weight\n",
      "_orig_mod.blocks.0.sa.heads.5.query.weight\n",
      "_orig_mod.blocks.0.sa.heads.5.value.weight\n",
      "_orig_mod.blocks.0.sa.proj.weight\n",
      "_orig_mod.blocks.0.sa.proj.bias\n",
      "_orig_mod.blocks.0.ffwd.net.0.weight\n",
      "_orig_mod.blocks.0.ffwd.net.2.weight\n",
      "_orig_mod.blocks.0.ln1.weight\n",
      "_orig_mod.blocks.0.ln2.weight\n",
      "_orig_mod.blocks.1.sa.heads.0.lora_query_matrix_B\n",
      "_orig_mod.blocks.1.sa.heads.0.lora_query_matrix_A\n",
      "_orig_mod.blocks.1.sa.heads.0.lora_value_matrix_B\n",
      "_orig_mod.blocks.1.sa.heads.0.lora_value_matrix_A\n",
      "_orig_mod.blocks.1.sa.heads.0.key.weight\n",
      "_orig_mod.blocks.1.sa.heads.0.query.weight\n",
      "_orig_mod.blocks.1.sa.heads.0.value.weight\n",
      "_orig_mod.blocks.1.sa.heads.1.lora_query_matrix_B\n",
      "_orig_mod.blocks.1.sa.heads.1.lora_query_matrix_A\n",
      "_orig_mod.blocks.1.sa.heads.1.lora_value_matrix_B\n",
      "_orig_mod.blocks.1.sa.heads.1.lora_value_matrix_A\n",
      "_orig_mod.blocks.1.sa.heads.1.key.weight\n",
      "_orig_mod.blocks.1.sa.heads.1.query.weight\n",
      "_orig_mod.blocks.1.sa.heads.1.value.weight\n",
      "_orig_mod.blocks.1.sa.heads.2.lora_query_matrix_B\n",
      "_orig_mod.blocks.1.sa.heads.2.lora_query_matrix_A\n",
      "_orig_mod.blocks.1.sa.heads.2.lora_value_matrix_B\n",
      "_orig_mod.blocks.1.sa.heads.2.lora_value_matrix_A\n",
      "_orig_mod.blocks.1.sa.heads.2.key.weight\n",
      "_orig_mod.blocks.1.sa.heads.2.query.weight\n",
      "_orig_mod.blocks.1.sa.heads.2.value.weight\n",
      "_orig_mod.blocks.1.sa.heads.3.lora_query_matrix_B\n",
      "_orig_mod.blocks.1.sa.heads.3.lora_query_matrix_A\n",
      "_orig_mod.blocks.1.sa.heads.3.lora_value_matrix_B\n",
      "_orig_mod.blocks.1.sa.heads.3.lora_value_matrix_A\n",
      "_orig_mod.blocks.1.sa.heads.3.key.weight\n",
      "_orig_mod.blocks.1.sa.heads.3.query.weight\n",
      "_orig_mod.blocks.1.sa.heads.3.value.weight\n",
      "_orig_mod.blocks.1.sa.heads.4.lora_query_matrix_B\n",
      "_orig_mod.blocks.1.sa.heads.4.lora_query_matrix_A\n",
      "_orig_mod.blocks.1.sa.heads.4.lora_value_matrix_B\n",
      "_orig_mod.blocks.1.sa.heads.4.lora_value_matrix_A\n",
      "_orig_mod.blocks.1.sa.heads.4.key.weight\n",
      "_orig_mod.blocks.1.sa.heads.4.query.weight\n",
      "_orig_mod.blocks.1.sa.heads.4.value.weight\n",
      "_orig_mod.blocks.1.sa.heads.5.lora_query_matrix_B\n",
      "_orig_mod.blocks.1.sa.heads.5.lora_query_matrix_A\n",
      "_orig_mod.blocks.1.sa.heads.5.lora_value_matrix_B\n",
      "_orig_mod.blocks.1.sa.heads.5.lora_value_matrix_A\n",
      "_orig_mod.blocks.1.sa.heads.5.key.weight\n",
      "_orig_mod.blocks.1.sa.heads.5.query.weight\n",
      "_orig_mod.blocks.1.sa.heads.5.value.weight\n",
      "_orig_mod.blocks.1.sa.proj.weight\n",
      "_orig_mod.blocks.1.sa.proj.bias\n",
      "_orig_mod.blocks.1.ffwd.net.0.weight\n",
      "_orig_mod.blocks.1.ffwd.net.2.weight\n",
      "_orig_mod.blocks.1.ln1.weight\n",
      "_orig_mod.blocks.1.ln2.weight\n",
      "_orig_mod.blocks.2.sa.heads.0.lora_query_matrix_B\n",
      "_orig_mod.blocks.2.sa.heads.0.lora_query_matrix_A\n",
      "_orig_mod.blocks.2.sa.heads.0.lora_value_matrix_B\n",
      "_orig_mod.blocks.2.sa.heads.0.lora_value_matrix_A\n",
      "_orig_mod.blocks.2.sa.heads.0.key.weight\n",
      "_orig_mod.blocks.2.sa.heads.0.query.weight\n",
      "_orig_mod.blocks.2.sa.heads.0.value.weight\n",
      "_orig_mod.blocks.2.sa.heads.1.lora_query_matrix_B\n",
      "_orig_mod.blocks.2.sa.heads.1.lora_query_matrix_A\n",
      "_orig_mod.blocks.2.sa.heads.1.lora_value_matrix_B\n",
      "_orig_mod.blocks.2.sa.heads.1.lora_value_matrix_A\n",
      "_orig_mod.blocks.2.sa.heads.1.key.weight\n",
      "_orig_mod.blocks.2.sa.heads.1.query.weight\n",
      "_orig_mod.blocks.2.sa.heads.1.value.weight\n",
      "_orig_mod.blocks.2.sa.heads.2.lora_query_matrix_B\n",
      "_orig_mod.blocks.2.sa.heads.2.lora_query_matrix_A\n",
      "_orig_mod.blocks.2.sa.heads.2.lora_value_matrix_B\n",
      "_orig_mod.blocks.2.sa.heads.2.lora_value_matrix_A\n",
      "_orig_mod.blocks.2.sa.heads.2.key.weight\n",
      "_orig_mod.blocks.2.sa.heads.2.query.weight\n",
      "_orig_mod.blocks.2.sa.heads.2.value.weight\n",
      "_orig_mod.blocks.2.sa.heads.3.lora_query_matrix_B\n",
      "_orig_mod.blocks.2.sa.heads.3.lora_query_matrix_A\n",
      "_orig_mod.blocks.2.sa.heads.3.lora_value_matrix_B\n",
      "_orig_mod.blocks.2.sa.heads.3.lora_value_matrix_A\n",
      "_orig_mod.blocks.2.sa.heads.3.key.weight\n",
      "_orig_mod.blocks.2.sa.heads.3.query.weight\n",
      "_orig_mod.blocks.2.sa.heads.3.value.weight\n",
      "_orig_mod.blocks.2.sa.heads.4.lora_query_matrix_B\n",
      "_orig_mod.blocks.2.sa.heads.4.lora_query_matrix_A\n",
      "_orig_mod.blocks.2.sa.heads.4.lora_value_matrix_B\n",
      "_orig_mod.blocks.2.sa.heads.4.lora_value_matrix_A\n",
      "_orig_mod.blocks.2.sa.heads.4.key.weight\n",
      "_orig_mod.blocks.2.sa.heads.4.query.weight\n",
      "_orig_mod.blocks.2.sa.heads.4.value.weight\n",
      "_orig_mod.blocks.2.sa.heads.5.lora_query_matrix_B\n",
      "_orig_mod.blocks.2.sa.heads.5.lora_query_matrix_A\n",
      "_orig_mod.blocks.2.sa.heads.5.lora_value_matrix_B\n",
      "_orig_mod.blocks.2.sa.heads.5.lora_value_matrix_A\n",
      "_orig_mod.blocks.2.sa.heads.5.key.weight\n",
      "_orig_mod.blocks.2.sa.heads.5.query.weight\n",
      "_orig_mod.blocks.2.sa.heads.5.value.weight\n",
      "_orig_mod.blocks.2.sa.proj.weight\n",
      "_orig_mod.blocks.2.sa.proj.bias\n",
      "_orig_mod.blocks.2.ffwd.net.0.weight\n",
      "_orig_mod.blocks.2.ffwd.net.2.weight\n",
      "_orig_mod.blocks.2.ln1.weight\n",
      "_orig_mod.blocks.2.ln2.weight\n",
      "_orig_mod.blocks.3.sa.heads.0.lora_query_matrix_B\n",
      "_orig_mod.blocks.3.sa.heads.0.lora_query_matrix_A\n",
      "_orig_mod.blocks.3.sa.heads.0.lora_value_matrix_B\n",
      "_orig_mod.blocks.3.sa.heads.0.lora_value_matrix_A\n",
      "_orig_mod.blocks.3.sa.heads.0.key.weight\n",
      "_orig_mod.blocks.3.sa.heads.0.query.weight\n",
      "_orig_mod.blocks.3.sa.heads.0.value.weight\n",
      "_orig_mod.blocks.3.sa.heads.1.lora_query_matrix_B\n",
      "_orig_mod.blocks.3.sa.heads.1.lora_query_matrix_A\n",
      "_orig_mod.blocks.3.sa.heads.1.lora_value_matrix_B\n",
      "_orig_mod.blocks.3.sa.heads.1.lora_value_matrix_A\n",
      "_orig_mod.blocks.3.sa.heads.1.key.weight\n",
      "_orig_mod.blocks.3.sa.heads.1.query.weight\n",
      "_orig_mod.blocks.3.sa.heads.1.value.weight\n",
      "_orig_mod.blocks.3.sa.heads.2.lora_query_matrix_B\n",
      "_orig_mod.blocks.3.sa.heads.2.lora_query_matrix_A\n",
      "_orig_mod.blocks.3.sa.heads.2.lora_value_matrix_B\n",
      "_orig_mod.blocks.3.sa.heads.2.lora_value_matrix_A\n",
      "_orig_mod.blocks.3.sa.heads.2.key.weight\n",
      "_orig_mod.blocks.3.sa.heads.2.query.weight\n",
      "_orig_mod.blocks.3.sa.heads.2.value.weight\n",
      "_orig_mod.blocks.3.sa.heads.3.lora_query_matrix_B\n",
      "_orig_mod.blocks.3.sa.heads.3.lora_query_matrix_A\n",
      "_orig_mod.blocks.3.sa.heads.3.lora_value_matrix_B\n",
      "_orig_mod.blocks.3.sa.heads.3.lora_value_matrix_A\n",
      "_orig_mod.blocks.3.sa.heads.3.key.weight\n",
      "_orig_mod.blocks.3.sa.heads.3.query.weight\n",
      "_orig_mod.blocks.3.sa.heads.3.value.weight\n",
      "_orig_mod.blocks.3.sa.heads.4.lora_query_matrix_B\n",
      "_orig_mod.blocks.3.sa.heads.4.lora_query_matrix_A\n",
      "_orig_mod.blocks.3.sa.heads.4.lora_value_matrix_B\n",
      "_orig_mod.blocks.3.sa.heads.4.lora_value_matrix_A\n",
      "_orig_mod.blocks.3.sa.heads.4.key.weight\n",
      "_orig_mod.blocks.3.sa.heads.4.query.weight\n",
      "_orig_mod.blocks.3.sa.heads.4.value.weight\n",
      "_orig_mod.blocks.3.sa.heads.5.lora_query_matrix_B\n",
      "_orig_mod.blocks.3.sa.heads.5.lora_query_matrix_A\n",
      "_orig_mod.blocks.3.sa.heads.5.lora_value_matrix_B\n",
      "_orig_mod.blocks.3.sa.heads.5.lora_value_matrix_A\n",
      "_orig_mod.blocks.3.sa.heads.5.key.weight\n",
      "_orig_mod.blocks.3.sa.heads.5.query.weight\n",
      "_orig_mod.blocks.3.sa.heads.5.value.weight\n",
      "_orig_mod.blocks.3.sa.proj.weight\n",
      "_orig_mod.blocks.3.sa.proj.bias\n",
      "_orig_mod.blocks.3.ffwd.net.0.weight\n",
      "_orig_mod.blocks.3.ffwd.net.2.weight\n",
      "_orig_mod.blocks.3.ln1.weight\n",
      "_orig_mod.blocks.3.ln2.weight\n",
      "_orig_mod.blocks.4.sa.heads.0.lora_query_matrix_B\n",
      "_orig_mod.blocks.4.sa.heads.0.lora_query_matrix_A\n",
      "_orig_mod.blocks.4.sa.heads.0.lora_value_matrix_B\n",
      "_orig_mod.blocks.4.sa.heads.0.lora_value_matrix_A\n",
      "_orig_mod.blocks.4.sa.heads.0.key.weight\n",
      "_orig_mod.blocks.4.sa.heads.0.query.weight\n",
      "_orig_mod.blocks.4.sa.heads.0.value.weight\n",
      "_orig_mod.blocks.4.sa.heads.1.lora_query_matrix_B\n",
      "_orig_mod.blocks.4.sa.heads.1.lora_query_matrix_A\n",
      "_orig_mod.blocks.4.sa.heads.1.lora_value_matrix_B\n",
      "_orig_mod.blocks.4.sa.heads.1.lora_value_matrix_A\n",
      "_orig_mod.blocks.4.sa.heads.1.key.weight\n",
      "_orig_mod.blocks.4.sa.heads.1.query.weight\n",
      "_orig_mod.blocks.4.sa.heads.1.value.weight\n",
      "_orig_mod.blocks.4.sa.heads.2.lora_query_matrix_B\n",
      "_orig_mod.blocks.4.sa.heads.2.lora_query_matrix_A\n",
      "_orig_mod.blocks.4.sa.heads.2.lora_value_matrix_B\n",
      "_orig_mod.blocks.4.sa.heads.2.lora_value_matrix_A\n",
      "_orig_mod.blocks.4.sa.heads.2.key.weight\n",
      "_orig_mod.blocks.4.sa.heads.2.query.weight\n",
      "_orig_mod.blocks.4.sa.heads.2.value.weight\n",
      "_orig_mod.blocks.4.sa.heads.3.lora_query_matrix_B\n",
      "_orig_mod.blocks.4.sa.heads.3.lora_query_matrix_A\n",
      "_orig_mod.blocks.4.sa.heads.3.lora_value_matrix_B\n",
      "_orig_mod.blocks.4.sa.heads.3.lora_value_matrix_A\n",
      "_orig_mod.blocks.4.sa.heads.3.key.weight\n",
      "_orig_mod.blocks.4.sa.heads.3.query.weight\n",
      "_orig_mod.blocks.4.sa.heads.3.value.weight\n",
      "_orig_mod.blocks.4.sa.heads.4.lora_query_matrix_B\n",
      "_orig_mod.blocks.4.sa.heads.4.lora_query_matrix_A\n",
      "_orig_mod.blocks.4.sa.heads.4.lora_value_matrix_B\n",
      "_orig_mod.blocks.4.sa.heads.4.lora_value_matrix_A\n",
      "_orig_mod.blocks.4.sa.heads.4.key.weight\n",
      "_orig_mod.blocks.4.sa.heads.4.query.weight\n",
      "_orig_mod.blocks.4.sa.heads.4.value.weight\n",
      "_orig_mod.blocks.4.sa.heads.5.lora_query_matrix_B\n",
      "_orig_mod.blocks.4.sa.heads.5.lora_query_matrix_A\n",
      "_orig_mod.blocks.4.sa.heads.5.lora_value_matrix_B\n",
      "_orig_mod.blocks.4.sa.heads.5.lora_value_matrix_A\n",
      "_orig_mod.blocks.4.sa.heads.5.key.weight\n",
      "_orig_mod.blocks.4.sa.heads.5.query.weight\n",
      "_orig_mod.blocks.4.sa.heads.5.value.weight\n",
      "_orig_mod.blocks.4.sa.proj.weight\n",
      "_orig_mod.blocks.4.sa.proj.bias\n",
      "_orig_mod.blocks.4.ffwd.net.0.weight\n",
      "_orig_mod.blocks.4.ffwd.net.2.weight\n",
      "_orig_mod.blocks.4.ln1.weight\n",
      "_orig_mod.blocks.4.ln2.weight\n",
      "_orig_mod.blocks.5.sa.heads.0.lora_query_matrix_B\n",
      "_orig_mod.blocks.5.sa.heads.0.lora_query_matrix_A\n",
      "_orig_mod.blocks.5.sa.heads.0.lora_value_matrix_B\n",
      "_orig_mod.blocks.5.sa.heads.0.lora_value_matrix_A\n",
      "_orig_mod.blocks.5.sa.heads.0.key.weight\n",
      "_orig_mod.blocks.5.sa.heads.0.query.weight\n",
      "_orig_mod.blocks.5.sa.heads.0.value.weight\n",
      "_orig_mod.blocks.5.sa.heads.1.lora_query_matrix_B\n",
      "_orig_mod.blocks.5.sa.heads.1.lora_query_matrix_A\n",
      "_orig_mod.blocks.5.sa.heads.1.lora_value_matrix_B\n",
      "_orig_mod.blocks.5.sa.heads.1.lora_value_matrix_A\n",
      "_orig_mod.blocks.5.sa.heads.1.key.weight\n",
      "_orig_mod.blocks.5.sa.heads.1.query.weight\n",
      "_orig_mod.blocks.5.sa.heads.1.value.weight\n",
      "_orig_mod.blocks.5.sa.heads.2.lora_query_matrix_B\n",
      "_orig_mod.blocks.5.sa.heads.2.lora_query_matrix_A\n",
      "_orig_mod.blocks.5.sa.heads.2.lora_value_matrix_B\n",
      "_orig_mod.blocks.5.sa.heads.2.lora_value_matrix_A\n",
      "_orig_mod.blocks.5.sa.heads.2.key.weight\n",
      "_orig_mod.blocks.5.sa.heads.2.query.weight\n",
      "_orig_mod.blocks.5.sa.heads.2.value.weight\n",
      "_orig_mod.blocks.5.sa.heads.3.lora_query_matrix_B\n",
      "_orig_mod.blocks.5.sa.heads.3.lora_query_matrix_A\n",
      "_orig_mod.blocks.5.sa.heads.3.lora_value_matrix_B\n",
      "_orig_mod.blocks.5.sa.heads.3.lora_value_matrix_A\n",
      "_orig_mod.blocks.5.sa.heads.3.key.weight\n",
      "_orig_mod.blocks.5.sa.heads.3.query.weight\n",
      "_orig_mod.blocks.5.sa.heads.3.value.weight\n",
      "_orig_mod.blocks.5.sa.heads.4.lora_query_matrix_B\n",
      "_orig_mod.blocks.5.sa.heads.4.lora_query_matrix_A\n",
      "_orig_mod.blocks.5.sa.heads.4.lora_value_matrix_B\n",
      "_orig_mod.blocks.5.sa.heads.4.lora_value_matrix_A\n",
      "_orig_mod.blocks.5.sa.heads.4.key.weight\n",
      "_orig_mod.blocks.5.sa.heads.4.query.weight\n",
      "_orig_mod.blocks.5.sa.heads.4.value.weight\n",
      "_orig_mod.blocks.5.sa.heads.5.lora_query_matrix_B\n",
      "_orig_mod.blocks.5.sa.heads.5.lora_query_matrix_A\n",
      "_orig_mod.blocks.5.sa.heads.5.lora_value_matrix_B\n",
      "_orig_mod.blocks.5.sa.heads.5.lora_value_matrix_A\n",
      "_orig_mod.blocks.5.sa.heads.5.key.weight\n",
      "_orig_mod.blocks.5.sa.heads.5.query.weight\n",
      "_orig_mod.blocks.5.sa.heads.5.value.weight\n",
      "_orig_mod.blocks.5.sa.proj.weight\n",
      "_orig_mod.blocks.5.sa.proj.bias\n",
      "_orig_mod.blocks.5.ffwd.net.0.weight\n",
      "_orig_mod.blocks.5.ffwd.net.2.weight\n",
      "_orig_mod.blocks.5.ln1.weight\n",
      "_orig_mod.blocks.5.ln2.weight\n",
      "_orig_mod.ln_f.weight\n",
      "_orig_mod.lm_head.weight\n",
      "_orig_mod.lm_head.bias\n",
      "Compiling the model...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lora_model import LoraGPT\n",
    "batch_size = 256\n",
    "eval_iters = 1000\n",
    "import torch\n",
    "model_path = \"./10M_2024-07-21_08-16.pth\"\n",
    "\n",
    "\n",
    "model = LoraGPT(r=10,device=device)\n",
    "print(\"Compiling the model...\\n\")\n",
    "try:\n",
    "    model = torch.compile(model)  # requires PyTorch 2.0\n",
    "except Exception as e:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10105433\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(num_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get random batch of data\n",
    "import model_def as m\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - m.block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+m.block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+m.block_size]).astype(np.int64)) for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# Estimate loss on train and val splits\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters) \n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "# Helper function to make large numbers of parameters human-readable\n",
    "def human_readable(num):\n",
    "    magnitude = 0\n",
    "    while abs(num) >= 1000:\n",
    "        magnitude += 1\n",
    "        num /= 1000.0\n",
    "    return '%.0f%s' % (num, ['', 'K', 'M', 'G', 'T', 'P'][magnitude])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "max_iters = 60000  # Maximum number of iterations\n",
    "learning_rate = 1e-3 # Initial Learning rate value\n",
    "miles = [int(max_iters * m) for m in [0.7, 0.8, 0.9]]  # Milestones for learning rate decay as fractions of max_iters\n",
    "eval_interval = 10000 # Evaluation interval\n",
    "eval_iters = 500  # Number of iterations for evaluation\n",
    "batch_size=256\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initialize learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=miles, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# evaluate the model on the train and val splits and log the losses\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m eval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 14\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m5d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# train the model for one iteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[71], line 21\u001b[0m, in \u001b[0;36mestimate_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_iters):\n\u001b[1;32m     20\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m get_batch(split)\n\u001b[0;32m---> 21\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     losses[k] \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     23\u001b[0m out[split] \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:433\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    429\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[1;32m    430\u001b[0m )\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[1;32m    437\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[1;32m    438\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/nyuad/Tiny-Language-Models-Framework/data/tasks_generators/model_def.py:100\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(n_embd, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(n_embd, vocab_size)\n\u001b[0;32m--> 100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx, targets\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    101\u001b[0m     B, T \u001b[38;5;241m=\u001b[39m idx\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m# idx and targets are both (B,T) tensor of integers\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:600\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    602\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:987\u001b[0m, in \u001b[0;36maot_module_simplified.<locals>.forward\u001b[0;34m(*runtime_args)\u001b[0m\n\u001b[1;32m    985\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(params_flat)\n\u001b[1;32m    986\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(runtime_args)\n\u001b[0;32m--> 987\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:217\u001b[0m, in \u001b[0;36m_create_runtime_wrapper.<locals>.runtime_wrapper\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m grad_enabled:\n\u001b[1;32m    216\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_set_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 217\u001b[0m     all_outs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompiled_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteal_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m grad_enabled:\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py:120\u001b[0m, in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 120\u001b[0m         out \u001b[38;5;241m=\u001b[39m normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m    124\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt take boxed arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    127\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    128\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:451\u001b[0m, in \u001b[0;36mFunctionalizedRngRuntimeWrapper.post_compile.<locals>.wrapper\u001b[0;34m(runtime_args)\u001b[0m\n\u001b[1;32m    444\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_functionalized_rng_runtime_epilogue(\n\u001b[1;32m    445\u001b[0m         runtime_metadata,\n\u001b[1;32m    446\u001b[0m         out,\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;66;03m# TODO: this won't be right for the backward when we convert the call_compiled_backward to use the wrapper\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         runtime_metadata\u001b[38;5;241m.\u001b[39mnum_forward_returns,\n\u001b[1;32m    449\u001b[0m     )\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruntime_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/_inductor/codecache.py:1131\u001b[0m, in \u001b[0;36mCompiledFxGraph.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: List[Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/torchinductor_raid/j5/cj5tr47e567tn5russfqbpchun56wmmpfqdzlqfwhrr42cwiqeto.py:8703\u001b[0m, in \u001b[0;36mcall\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m   8701\u001b[0m buf473 \u001b[38;5;241m=\u001b[39m reinterpret_tensor(buf468, (\u001b[38;5;241m65536\u001b[39m, \u001b[38;5;241m62\u001b[39m), (\u001b[38;5;241m62\u001b[39m, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m0\u001b[39m); \u001b[38;5;28;01mdel\u001b[39;00m buf468  \u001b[38;5;66;03m# reuse\u001b[39;00m\n\u001b[1;32m   8702\u001b[0m \u001b[38;5;66;03m# Source Nodes: [v_33], Original ATen: [aten.mm]\u001b[39;00m\n\u001b[0;32m-> 8703\u001b[0m \u001b[43mextern_kernels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreinterpret_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf431\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m65536\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m372\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m372\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreinterpret_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg134_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m372\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m62\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m372\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuf473\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   8704\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m arg134_1\n\u001b[1;32m   8705\u001b[0m buf475 \u001b[38;5;241m=\u001b[39m buf467; \u001b[38;5;28;01mdel\u001b[39;00m buf467  \u001b[38;5;66;03m# reuse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get current date and hour to get track of experiments\n",
    "now = datetime.datetime.now()\n",
    "date_hour = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "# Train\n",
    "# Start training timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Training loop\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # evaluate the model on the train and val splits and log the losses\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f'iter {iter:5d} | train loss {losses[\"train\"]:.4f} | val loss {losses[\"val\"]:.4f}')\n",
    "        \n",
    "    # train the model for one iteration\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # forward pass\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Step the scheduler\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "# End training timer\n",
    "end_time = time.time()\n",
    "print(f'Training time: {(end_time - start_time) / 60}  min')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), f\"{num_parameters}_{date_hour}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.memmap('test.bin', dtype=np.uint16, mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_example(example, model, max_new_tokens=30):\n",
    "\n",
    "    # Split example and determine maximum new tokens allowed\n",
    "    splited_example = example.split(\"# reformulation\")\n",
    "    if not (\"for\" in splited_example[0]):\n",
    "        max_new_tokens = 22\n",
    "    # Encode prompt and prepare for evaluation\n",
    "    encoded_example = torch.tensor(encode(splited_example[0] + \"# reformulation\"), dtype=torch.long).unsqueeze(0).to(device)\n",
    "    prompt_text = splited_example[0] + \"# reformulation\"\n",
    "\n",
    "    result_example = splited_example[-1]\n",
    "\n",
    "    #print(\"result: ==>\",result_example)\n",
    "\n",
    "    # Extract real results from example\n",
    "    #real_results = [float(match.group()) for match in re.finditer(r\"(?<=# )-?\\d+(\\.\\d+)?\", result_example.split('\\n\\n')[0].replace(\"\\n\", \"\"))]\n",
    "\n",
    "    # Generate response from model and extract generated results\n",
    "    response = decode(model.generate(encoded_example, max_new_tokens=max_new_tokens)[0].tolist())\n",
    "    splited_response = response.split(\"# reformulation\")\n",
    "    result_response = splited_response[-1]\n",
    "    #generated_results = [float(match.group()) for match in re.finditer(r\"(?<=# )-?\\d+(\\.\\d+)?\", result_response.split('\\n\\n')[0].replace(\"\\n\", \"\"))]\n",
    "\n",
    "    return prompt_text, result_example, result_response\n",
    "\n",
    "\n",
    "\n",
    "# Write results to file\n",
    "def write_results_to_file(output_file, prompt, real_results, generated_results):\n",
    "    df = pd.DataFrame({\n",
    "        'Prompt': prompt,\n",
    "        'Real_Results': real_results,\n",
    "        'Generated_Results': generated_results\n",
    "    })\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_pair(real, generated_result):\n",
    "    # Determine the length of the shorter and longer strings\n",
    "    min_len = min(len(real), len(generated_result))\n",
    "    max_len = max(len(real), len(generated_result))\n",
    "\n",
    "    # Count the number of matching characters at the same index\n",
    "    match_count = sum(1 for i in range(min_len) if real[i] == generated_result[i])\n",
    "\n",
    "    # Calculate the ratio of matches to the length of the longer string\n",
    "    ratio = match_count / max_len\n",
    "    return ratio\n",
    "\n",
    "# Evaluation Loop\n",
    "\n",
    "# Split examples and initialize lists for results\n",
    "examples = decode(test_data).split(\"\\n\\n\")\n",
    "examples = [example for example in examples if example]\n",
    "\n",
    "# Start evaluation process\n",
    "prompt = []\n",
    "real_results = []\n",
    "generated_results = []\n",
    "\n",
    "# Iterate through examples and evaluate the model on each one\n",
    "for example in tqdm(examples):\n",
    "    prompt_text, real_result, result = evaluate_example(example, model)\n",
    "    prompt.append(prompt_text)\n",
    "    real_results.append(real_result)\n",
    "    generated_results.append(result)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "score=0\n",
    "\n",
    "for real,generated in zip(real_results, generated_results):\n",
    "  score+=evaluate_pair(real,generated)\n",
    "accuracy = score / len(generated_results)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Store accuracy in a file\n",
    "with open(\"accuracy.txt\", 'w') as f:\n",
    "    f.write(f\"Accuracy: {accuracy * 100:.2f}%\\n\")\n",
    "\n",
    "# Store predictions in a CSV file\n",
    "    write_results_to_file(\"predictions.csv\", prompt, real_results, generated_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
