{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to cpu.\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Set the device to GPU if available, otherwise CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device set to {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to load and save data\n",
    "def save_data(data, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(data)\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR=\"./\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found vocab_size = 36 (inside ./meta.pkl)\n"
     ]
    }
   ],
   "source": [
    "# Attempt to derive vocab_size from the dataset\n",
    "\n",
    "meta_path = os.path.join(DATA_DIR, 'meta.pkl')\n",
    "vocab_size = None\n",
    "\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    vocab_size = meta['vocab_size']\n",
    "    print(f\"found vocab_size = {vocab_size} (inside {meta_path})\")\n",
    "else:\n",
    "    print(\"Meta file not found. Please ensure the meta.pkl file is present in the data directory.\")\n",
    "\n",
    "# Encode and decode functions for character-level Tokenzation \n",
    "def encode(s):\n",
    "    return [meta['stoi'][c] for c in s]\n",
    "\n",
    "def decode(l):\n",
    "    return ''.join([meta['itos'][i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded data saved as binary files.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_data = load_data(os.path.join(DATA_DIR, 'train.txt'))\n",
    "val_data = load_data(os.path.join(DATA_DIR, 'val.txt'))\n",
    "test_data = load_data(os.path.join(DATA_DIR, 'test.txt'))\n",
    "\n",
    "# Encode data\n",
    "train_ids = encode(train_data)\n",
    "val_ids = encode(val_data)\n",
    "test_ids = encode(test_data)\n",
    "\n",
    "# Save encoded data to bin files, make sure to choose \"Files only\" on the persistence option of the session so that you don't encode data each time\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "test_ids = np.array(test_ids, dtype=np.uint16)\n",
    "\n",
    "train_ids.tofile( 'train.bin')\n",
    "val_ids.tofile( 'val.bin')\n",
    "test_ids.tofile('test.bin')\n",
    "\n",
    "print(\"Encoded data saved as binary files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(train_ids)\n",
    "del(val_ids)\n",
    "del(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load encoded data\n",
    "train_data = np.memmap(\"./train.bin\", dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(\"./val.bin\", dtype=np.uint16, mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "n_embd=372\n",
    "n_head=6\n",
    "n_layer=6\n",
    "dropout=0\n",
    "vocab_size=53\n",
    "block_size=256\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self-attention.\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B, T, 16)\n",
    "        q = self.query(x) # (B, T, 16)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        out = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=dropout if self.training else 0, is_causal=True)\n",
    "            \n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"multiple heads of self-attention in parallel.\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity.\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd, bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Linear( 4 * n_embd, n_embd, bias=False),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by feedforward.\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd, bias=False)\n",
    "        self.ln2 = nn.LayerNorm(n_embd, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd, bias=False) \n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:] # (B, T)\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LoraHead(Head):\n",
    "    \"\"\"\n",
    "    Extends MultiHeadAttention with LoRA (Low-Rank Adaptation) matrices.\n",
    "    LoRA enhances efficiency by only updating the query and value matrices.\n",
    "    This class adds LoRA matrices and applies LoRA logic in the forward method.\n",
    "\n",
    "    Parameters:\n",
    "    - r (int): Rank for LoRA matrices.\n",
    "    - config: Configuration of the Roberta Model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, r=8):\n",
    "        head_size = n_embd // n_head\n",
    "        super().__init__(head_size=head_size)\n",
    "        \n",
    "        self.lora_query_matrix_A = nn.Parameter(torch.zeros(r, head_size))\n",
    "        self.lora_query_matrix_B = nn.Parameter(torch.randn(n_embd, r))\n",
    "        self.lora_value_matrix_A = nn.Parameter(torch.zeros(r, head_size))\n",
    "        self.lora_value_matrix_B = nn.Parameter(torch.randn(n_embd, r))\n",
    "        self.lora_key_matrix_A = nn.Parameter(torch.zeros(r, head_size))\n",
    "        self.lora_key_matrix_B = nn.Parameter(torch.randn(n_embd, r))\n",
    "\n",
    "    def lora_query(self, x):\n",
    "        \"\"\"\n",
    "        Applies LoRA to the query component. Computes a modified query output by adding \n",
    "        the LoRA adaptation to the standard query output. Requires the regular linear layer \n",
    "        to be frozen before training.\n",
    "        \"\"\"\n",
    "        lora_query_weights = torch.matmul(self.lora_query_matrix_B, self.lora_query_matrix_A)\n",
    "\n",
    "        return self.query(x) + F.linear(x, lora_query_weights.T)\n",
    "\n",
    "    def lora_value(self, x):\n",
    "        \"\"\"\n",
    "        Applies LoRA to the value component. Computes a modified value output by adding \n",
    "        the LoRA adaptation to the standard value output. Requires the regular linear layer \n",
    "        to be frozen before training.\n",
    "        \"\"\"\n",
    "        lora_value_weights = torch.matmul(self.lora_value_matrix_B, self.lora_value_matrix_A)\n",
    "\n",
    "\n",
    "        return self.value(x) +  F.linear(x, lora_value_weights.T)\n",
    "\n",
    "    def lora_key(self, x):\n",
    "        \"\"\"\n",
    "        Applies LoRA to the key component. Computes a modified value output by adding \n",
    "        the LoRA adaptation to the standard value output. Requires the regular linear layer \n",
    "        to be frozen before training.\n",
    "        \"\"\"\n",
    "        lora_key_weights = torch.matmul(self.lora_key_matrix_B, self.lora_key_matrix_A)\n",
    "\n",
    "        \n",
    "        return self.key(x) +  F.linear(x, lora_key_weights.T)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.lora_key(x)\n",
    "        q = self.lora_query(x)\n",
    "        v = self.lora_value(x)\n",
    "\n",
    "        out = torch.nn.functional.scaled_dot_product_attention(\n",
    "            q, k, v, attn_mask=None, dropout_p=dropout if self.training else 0, is_causal=True\n",
    "        )\n",
    "\n",
    "        return out\n",
    "class LoraGPT(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __load_model(self)->GPT:\n",
    "        model_path = \"./10M_2024-07-21_08-16.pth\"\n",
    "\n",
    "\n",
    "        model = GPT()\n",
    "        print(\"Compiling the model...\\n\")\n",
    "        try:\n",
    "            model = torch.compile(model)  # requires PyTorch 2.0\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        model.load_state_dict(torch.load(model_path,map_location=self.device))\n",
    "\n",
    "        m = model.to(self.device)\n",
    "        return m\n",
    "    \n",
    "    def __init__(self,  r=8,device='cuda'):\n",
    "        \n",
    "        \n",
    "        super().__init__()\n",
    "        self.lora_rank = r\n",
    "        self.device = device\n",
    "        self.model=self.__load_model()\n",
    "        self.replace_multihead_attention_recursion(self.model)\n",
    "        self.freeze_parameters_except_lora_and_bias()\n",
    "        \n",
    "        \n",
    "    def forward(self, x,targets):\n",
    "        return self.model(x,targets)\n",
    "   \n",
    "        \n",
    "    def replace_multihead_attention_recursion(self,model):\n",
    "        \"\"\"\n",
    "        Replaces RobertaSelfAttention with LoraRobertaSelfAttention in the model.\n",
    "        This method applies the replacement recursively to all sub-components.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : nn.Module\n",
    "            The PyTorch module or model to be modified.\n",
    "        \"\"\"\n",
    "        for name, module in model.named_children():\n",
    "            if isinstance(module, Head):\n",
    "                # Replace RobertaSelfAttention with LoraRobertaSelfAttention\n",
    "                new_layer = LoraHead(r=self.lora_rank)\n",
    "                new_layer.load_state_dict(module.state_dict(), strict=False)\n",
    "                setattr(model, name, new_layer)\n",
    "            else:\n",
    "                # Recursive call for child modules\n",
    "                self.replace_multihead_attention_recursion(module)\n",
    "                \n",
    "                \n",
    "    def freeze_parameters_except_lora_and_bias(self):\n",
    "        \"\"\"\n",
    "        Freezes all model parameters except for specific layers and types based on the configuration.\n",
    "        Parameters in LoRA layers, the finetune head, bias parameters, embeddings, and layer norms \n",
    "        can be set as trainable based on class settings.\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            print(name)\n",
    "            is_trainable = (\n",
    "                \"lora_\" in name \n",
    "                \n",
    "                #(self.train_layer_norms and \"LayerNorm\" in name)\n",
    "            )\n",
    "            param.requires_grad = is_trainable\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the model...\n",
      "\n",
      "_orig_mod.token_embedding_table.weight\n",
      "_orig_mod.position_embedding_table.weight\n",
      "_orig_mod.blocks.0.sa.heads.0.lora_query_matrix_A\n",
      "_orig_mod.blocks.0.sa.heads.0.lora_query_matrix_B\n",
      "_orig_mod.blocks.0.sa.heads.0.lora_value_matrix_A\n",
      "_orig_mod.blocks.0.sa.heads.0.lora_value_matrix_B\n",
      "_orig_mod.blocks.0.sa.heads.0.lora_key_matrix_A\n",
      "_orig_mod.blocks.0.sa.heads.0.lora_key_matrix_B\n",
      "_orig_mod.blocks.0.sa.heads.0.key.weight\n",
      "_orig_mod.blocks.0.sa.heads.0.query.weight\n",
      "_orig_mod.blocks.0.sa.heads.0.value.weight\n",
      "_orig_mod.blocks.0.sa.heads.1.lora_query_matrix_A\n",
      "_orig_mod.blocks.0.sa.heads.1.lora_query_matrix_B\n",
      "_orig_mod.blocks.0.sa.heads.1.lora_value_matrix_A\n",
      "_orig_mod.blocks.0.sa.heads.1.lora_value_matrix_B\n",
      "_orig_mod.blocks.0.sa.heads.1.lora_key_matrix_A\n",
      "_orig_mod.blocks.0.sa.heads.1.lora_key_matrix_B\n",
      "_orig_mod.blocks.0.sa.heads.1.key.weight\n",
      "_orig_mod.blocks.0.sa.heads.1.query.weight\n",
      "_orig_mod.blocks.0.sa.heads.1.value.weight\n",
      "_orig_mod.blocks.0.sa.heads.2.lora_query_matrix_A\n",
      "_orig_mod.blocks.0.sa.heads.2.lora_query_matrix_B\n",
      "_orig_mod.blocks.0.sa.heads.2.lora_value_matrix_A\n",
      "_orig_mod.blocks.0.sa.heads.2.lora_value_matrix_B\n",
      "_orig_mod.blocks.0.sa.heads.2.lora_key_matrix_A\n",
      "_orig_mod.blocks.0.sa.heads.2.lora_key_matrix_B\n",
      "_orig_mod.blocks.0.sa.heads.2.key.weight\n",
      "_orig_mod.blocks.0.sa.heads.2.query.weight\n",
      "_orig_mod.blocks.0.sa.heads.2.value.weight\n",
      "_orig_mod.blocks.0.sa.heads.3.lora_query_matrix_A\n",
      "_orig_mod.blocks.0.sa.heads.3.lora_query_matrix_B\n",
      "_orig_mod.blocks.0.sa.heads.3.lora_value_matrix_A\n",
      "_orig_mod.blocks.0.sa.heads.3.lora_value_matrix_B\n",
      "_orig_mod.blocks.0.sa.heads.3.lora_key_matrix_A\n",
      "_orig_mod.blocks.0.sa.heads.3.lora_key_matrix_B\n",
      "_orig_mod.blocks.0.sa.heads.3.key.weight\n",
      "_orig_mod.blocks.0.sa.heads.3.query.weight\n",
      "_orig_mod.blocks.0.sa.heads.3.value.weight\n",
      "_orig_mod.blocks.0.sa.heads.4.lora_query_matrix_A\n",
      "_orig_mod.blocks.0.sa.heads.4.lora_query_matrix_B\n",
      "_orig_mod.blocks.0.sa.heads.4.lora_value_matrix_A\n",
      "_orig_mod.blocks.0.sa.heads.4.lora_value_matrix_B\n",
      "_orig_mod.blocks.0.sa.heads.4.lora_key_matrix_A\n",
      "_orig_mod.blocks.0.sa.heads.4.lora_key_matrix_B\n",
      "_orig_mod.blocks.0.sa.heads.4.key.weight\n",
      "_orig_mod.blocks.0.sa.heads.4.query.weight\n",
      "_orig_mod.blocks.0.sa.heads.4.value.weight\n",
      "_orig_mod.blocks.0.sa.heads.5.lora_query_matrix_A\n",
      "_orig_mod.blocks.0.sa.heads.5.lora_query_matrix_B\n",
      "_orig_mod.blocks.0.sa.heads.5.lora_value_matrix_A\n",
      "_orig_mod.blocks.0.sa.heads.5.lora_value_matrix_B\n",
      "_orig_mod.blocks.0.sa.heads.5.lora_key_matrix_A\n",
      "_orig_mod.blocks.0.sa.heads.5.lora_key_matrix_B\n",
      "_orig_mod.blocks.0.sa.heads.5.key.weight\n",
      "_orig_mod.blocks.0.sa.heads.5.query.weight\n",
      "_orig_mod.blocks.0.sa.heads.5.value.weight\n",
      "_orig_mod.blocks.0.sa.proj.weight\n",
      "_orig_mod.blocks.0.sa.proj.bias\n",
      "_orig_mod.blocks.0.ffwd.net.0.weight\n",
      "_orig_mod.blocks.0.ffwd.net.2.weight\n",
      "_orig_mod.blocks.0.ln1.weight\n",
      "_orig_mod.blocks.0.ln2.weight\n",
      "_orig_mod.blocks.1.sa.heads.0.lora_query_matrix_A\n",
      "_orig_mod.blocks.1.sa.heads.0.lora_query_matrix_B\n",
      "_orig_mod.blocks.1.sa.heads.0.lora_value_matrix_A\n",
      "_orig_mod.blocks.1.sa.heads.0.lora_value_matrix_B\n",
      "_orig_mod.blocks.1.sa.heads.0.lora_key_matrix_A\n",
      "_orig_mod.blocks.1.sa.heads.0.lora_key_matrix_B\n",
      "_orig_mod.blocks.1.sa.heads.0.key.weight\n",
      "_orig_mod.blocks.1.sa.heads.0.query.weight\n",
      "_orig_mod.blocks.1.sa.heads.0.value.weight\n",
      "_orig_mod.blocks.1.sa.heads.1.lora_query_matrix_A\n",
      "_orig_mod.blocks.1.sa.heads.1.lora_query_matrix_B\n",
      "_orig_mod.blocks.1.sa.heads.1.lora_value_matrix_A\n",
      "_orig_mod.blocks.1.sa.heads.1.lora_value_matrix_B\n",
      "_orig_mod.blocks.1.sa.heads.1.lora_key_matrix_A\n",
      "_orig_mod.blocks.1.sa.heads.1.lora_key_matrix_B\n",
      "_orig_mod.blocks.1.sa.heads.1.key.weight\n",
      "_orig_mod.blocks.1.sa.heads.1.query.weight\n",
      "_orig_mod.blocks.1.sa.heads.1.value.weight\n",
      "_orig_mod.blocks.1.sa.heads.2.lora_query_matrix_A\n",
      "_orig_mod.blocks.1.sa.heads.2.lora_query_matrix_B\n",
      "_orig_mod.blocks.1.sa.heads.2.lora_value_matrix_A\n",
      "_orig_mod.blocks.1.sa.heads.2.lora_value_matrix_B\n",
      "_orig_mod.blocks.1.sa.heads.2.lora_key_matrix_A\n",
      "_orig_mod.blocks.1.sa.heads.2.lora_key_matrix_B\n",
      "_orig_mod.blocks.1.sa.heads.2.key.weight\n",
      "_orig_mod.blocks.1.sa.heads.2.query.weight\n",
      "_orig_mod.blocks.1.sa.heads.2.value.weight\n",
      "_orig_mod.blocks.1.sa.heads.3.lora_query_matrix_A\n",
      "_orig_mod.blocks.1.sa.heads.3.lora_query_matrix_B\n",
      "_orig_mod.blocks.1.sa.heads.3.lora_value_matrix_A\n",
      "_orig_mod.blocks.1.sa.heads.3.lora_value_matrix_B\n",
      "_orig_mod.blocks.1.sa.heads.3.lora_key_matrix_A\n",
      "_orig_mod.blocks.1.sa.heads.3.lora_key_matrix_B\n",
      "_orig_mod.blocks.1.sa.heads.3.key.weight\n",
      "_orig_mod.blocks.1.sa.heads.3.query.weight\n",
      "_orig_mod.blocks.1.sa.heads.3.value.weight\n",
      "_orig_mod.blocks.1.sa.heads.4.lora_query_matrix_A\n",
      "_orig_mod.blocks.1.sa.heads.4.lora_query_matrix_B\n",
      "_orig_mod.blocks.1.sa.heads.4.lora_value_matrix_A\n",
      "_orig_mod.blocks.1.sa.heads.4.lora_value_matrix_B\n",
      "_orig_mod.blocks.1.sa.heads.4.lora_key_matrix_A\n",
      "_orig_mod.blocks.1.sa.heads.4.lora_key_matrix_B\n",
      "_orig_mod.blocks.1.sa.heads.4.key.weight\n",
      "_orig_mod.blocks.1.sa.heads.4.query.weight\n",
      "_orig_mod.blocks.1.sa.heads.4.value.weight\n",
      "_orig_mod.blocks.1.sa.heads.5.lora_query_matrix_A\n",
      "_orig_mod.blocks.1.sa.heads.5.lora_query_matrix_B\n",
      "_orig_mod.blocks.1.sa.heads.5.lora_value_matrix_A\n",
      "_orig_mod.blocks.1.sa.heads.5.lora_value_matrix_B\n",
      "_orig_mod.blocks.1.sa.heads.5.lora_key_matrix_A\n",
      "_orig_mod.blocks.1.sa.heads.5.lora_key_matrix_B\n",
      "_orig_mod.blocks.1.sa.heads.5.key.weight\n",
      "_orig_mod.blocks.1.sa.heads.5.query.weight\n",
      "_orig_mod.blocks.1.sa.heads.5.value.weight\n",
      "_orig_mod.blocks.1.sa.proj.weight\n",
      "_orig_mod.blocks.1.sa.proj.bias\n",
      "_orig_mod.blocks.1.ffwd.net.0.weight\n",
      "_orig_mod.blocks.1.ffwd.net.2.weight\n",
      "_orig_mod.blocks.1.ln1.weight\n",
      "_orig_mod.blocks.1.ln2.weight\n",
      "_orig_mod.blocks.2.sa.heads.0.lora_query_matrix_A\n",
      "_orig_mod.blocks.2.sa.heads.0.lora_query_matrix_B\n",
      "_orig_mod.blocks.2.sa.heads.0.lora_value_matrix_A\n",
      "_orig_mod.blocks.2.sa.heads.0.lora_value_matrix_B\n",
      "_orig_mod.blocks.2.sa.heads.0.lora_key_matrix_A\n",
      "_orig_mod.blocks.2.sa.heads.0.lora_key_matrix_B\n",
      "_orig_mod.blocks.2.sa.heads.0.key.weight\n",
      "_orig_mod.blocks.2.sa.heads.0.query.weight\n",
      "_orig_mod.blocks.2.sa.heads.0.value.weight\n",
      "_orig_mod.blocks.2.sa.heads.1.lora_query_matrix_A\n",
      "_orig_mod.blocks.2.sa.heads.1.lora_query_matrix_B\n",
      "_orig_mod.blocks.2.sa.heads.1.lora_value_matrix_A\n",
      "_orig_mod.blocks.2.sa.heads.1.lora_value_matrix_B\n",
      "_orig_mod.blocks.2.sa.heads.1.lora_key_matrix_A\n",
      "_orig_mod.blocks.2.sa.heads.1.lora_key_matrix_B\n",
      "_orig_mod.blocks.2.sa.heads.1.key.weight\n",
      "_orig_mod.blocks.2.sa.heads.1.query.weight\n",
      "_orig_mod.blocks.2.sa.heads.1.value.weight\n",
      "_orig_mod.blocks.2.sa.heads.2.lora_query_matrix_A\n",
      "_orig_mod.blocks.2.sa.heads.2.lora_query_matrix_B\n",
      "_orig_mod.blocks.2.sa.heads.2.lora_value_matrix_A\n",
      "_orig_mod.blocks.2.sa.heads.2.lora_value_matrix_B\n",
      "_orig_mod.blocks.2.sa.heads.2.lora_key_matrix_A\n",
      "_orig_mod.blocks.2.sa.heads.2.lora_key_matrix_B\n",
      "_orig_mod.blocks.2.sa.heads.2.key.weight\n",
      "_orig_mod.blocks.2.sa.heads.2.query.weight\n",
      "_orig_mod.blocks.2.sa.heads.2.value.weight\n",
      "_orig_mod.blocks.2.sa.heads.3.lora_query_matrix_A\n",
      "_orig_mod.blocks.2.sa.heads.3.lora_query_matrix_B\n",
      "_orig_mod.blocks.2.sa.heads.3.lora_value_matrix_A\n",
      "_orig_mod.blocks.2.sa.heads.3.lora_value_matrix_B\n",
      "_orig_mod.blocks.2.sa.heads.3.lora_key_matrix_A\n",
      "_orig_mod.blocks.2.sa.heads.3.lora_key_matrix_B\n",
      "_orig_mod.blocks.2.sa.heads.3.key.weight\n",
      "_orig_mod.blocks.2.sa.heads.3.query.weight\n",
      "_orig_mod.blocks.2.sa.heads.3.value.weight\n",
      "_orig_mod.blocks.2.sa.heads.4.lora_query_matrix_A\n",
      "_orig_mod.blocks.2.sa.heads.4.lora_query_matrix_B\n",
      "_orig_mod.blocks.2.sa.heads.4.lora_value_matrix_A\n",
      "_orig_mod.blocks.2.sa.heads.4.lora_value_matrix_B\n",
      "_orig_mod.blocks.2.sa.heads.4.lora_key_matrix_A\n",
      "_orig_mod.blocks.2.sa.heads.4.lora_key_matrix_B\n",
      "_orig_mod.blocks.2.sa.heads.4.key.weight\n",
      "_orig_mod.blocks.2.sa.heads.4.query.weight\n",
      "_orig_mod.blocks.2.sa.heads.4.value.weight\n",
      "_orig_mod.blocks.2.sa.heads.5.lora_query_matrix_A\n",
      "_orig_mod.blocks.2.sa.heads.5.lora_query_matrix_B\n",
      "_orig_mod.blocks.2.sa.heads.5.lora_value_matrix_A\n",
      "_orig_mod.blocks.2.sa.heads.5.lora_value_matrix_B\n",
      "_orig_mod.blocks.2.sa.heads.5.lora_key_matrix_A\n",
      "_orig_mod.blocks.2.sa.heads.5.lora_key_matrix_B\n",
      "_orig_mod.blocks.2.sa.heads.5.key.weight\n",
      "_orig_mod.blocks.2.sa.heads.5.query.weight\n",
      "_orig_mod.blocks.2.sa.heads.5.value.weight\n",
      "_orig_mod.blocks.2.sa.proj.weight\n",
      "_orig_mod.blocks.2.sa.proj.bias\n",
      "_orig_mod.blocks.2.ffwd.net.0.weight\n",
      "_orig_mod.blocks.2.ffwd.net.2.weight\n",
      "_orig_mod.blocks.2.ln1.weight\n",
      "_orig_mod.blocks.2.ln2.weight\n",
      "_orig_mod.blocks.3.sa.heads.0.lora_query_matrix_A\n",
      "_orig_mod.blocks.3.sa.heads.0.lora_query_matrix_B\n",
      "_orig_mod.blocks.3.sa.heads.0.lora_value_matrix_A\n",
      "_orig_mod.blocks.3.sa.heads.0.lora_value_matrix_B\n",
      "_orig_mod.blocks.3.sa.heads.0.lora_key_matrix_A\n",
      "_orig_mod.blocks.3.sa.heads.0.lora_key_matrix_B\n",
      "_orig_mod.blocks.3.sa.heads.0.key.weight\n",
      "_orig_mod.blocks.3.sa.heads.0.query.weight\n",
      "_orig_mod.blocks.3.sa.heads.0.value.weight\n",
      "_orig_mod.blocks.3.sa.heads.1.lora_query_matrix_A\n",
      "_orig_mod.blocks.3.sa.heads.1.lora_query_matrix_B\n",
      "_orig_mod.blocks.3.sa.heads.1.lora_value_matrix_A\n",
      "_orig_mod.blocks.3.sa.heads.1.lora_value_matrix_B\n",
      "_orig_mod.blocks.3.sa.heads.1.lora_key_matrix_A\n",
      "_orig_mod.blocks.3.sa.heads.1.lora_key_matrix_B\n",
      "_orig_mod.blocks.3.sa.heads.1.key.weight\n",
      "_orig_mod.blocks.3.sa.heads.1.query.weight\n",
      "_orig_mod.blocks.3.sa.heads.1.value.weight\n",
      "_orig_mod.blocks.3.sa.heads.2.lora_query_matrix_A\n",
      "_orig_mod.blocks.3.sa.heads.2.lora_query_matrix_B\n",
      "_orig_mod.blocks.3.sa.heads.2.lora_value_matrix_A\n",
      "_orig_mod.blocks.3.sa.heads.2.lora_value_matrix_B\n",
      "_orig_mod.blocks.3.sa.heads.2.lora_key_matrix_A\n",
      "_orig_mod.blocks.3.sa.heads.2.lora_key_matrix_B\n",
      "_orig_mod.blocks.3.sa.heads.2.key.weight\n",
      "_orig_mod.blocks.3.sa.heads.2.query.weight\n",
      "_orig_mod.blocks.3.sa.heads.2.value.weight\n",
      "_orig_mod.blocks.3.sa.heads.3.lora_query_matrix_A\n",
      "_orig_mod.blocks.3.sa.heads.3.lora_query_matrix_B\n",
      "_orig_mod.blocks.3.sa.heads.3.lora_value_matrix_A\n",
      "_orig_mod.blocks.3.sa.heads.3.lora_value_matrix_B\n",
      "_orig_mod.blocks.3.sa.heads.3.lora_key_matrix_A\n",
      "_orig_mod.blocks.3.sa.heads.3.lora_key_matrix_B\n",
      "_orig_mod.blocks.3.sa.heads.3.key.weight\n",
      "_orig_mod.blocks.3.sa.heads.3.query.weight\n",
      "_orig_mod.blocks.3.sa.heads.3.value.weight\n",
      "_orig_mod.blocks.3.sa.heads.4.lora_query_matrix_A\n",
      "_orig_mod.blocks.3.sa.heads.4.lora_query_matrix_B\n",
      "_orig_mod.blocks.3.sa.heads.4.lora_value_matrix_A\n",
      "_orig_mod.blocks.3.sa.heads.4.lora_value_matrix_B\n",
      "_orig_mod.blocks.3.sa.heads.4.lora_key_matrix_A\n",
      "_orig_mod.blocks.3.sa.heads.4.lora_key_matrix_B\n",
      "_orig_mod.blocks.3.sa.heads.4.key.weight\n",
      "_orig_mod.blocks.3.sa.heads.4.query.weight\n",
      "_orig_mod.blocks.3.sa.heads.4.value.weight\n",
      "_orig_mod.blocks.3.sa.heads.5.lora_query_matrix_A\n",
      "_orig_mod.blocks.3.sa.heads.5.lora_query_matrix_B\n",
      "_orig_mod.blocks.3.sa.heads.5.lora_value_matrix_A\n",
      "_orig_mod.blocks.3.sa.heads.5.lora_value_matrix_B\n",
      "_orig_mod.blocks.3.sa.heads.5.lora_key_matrix_A\n",
      "_orig_mod.blocks.3.sa.heads.5.lora_key_matrix_B\n",
      "_orig_mod.blocks.3.sa.heads.5.key.weight\n",
      "_orig_mod.blocks.3.sa.heads.5.query.weight\n",
      "_orig_mod.blocks.3.sa.heads.5.value.weight\n",
      "_orig_mod.blocks.3.sa.proj.weight\n",
      "_orig_mod.blocks.3.sa.proj.bias\n",
      "_orig_mod.blocks.3.ffwd.net.0.weight\n",
      "_orig_mod.blocks.3.ffwd.net.2.weight\n",
      "_orig_mod.blocks.3.ln1.weight\n",
      "_orig_mod.blocks.3.ln2.weight\n",
      "_orig_mod.blocks.4.sa.heads.0.lora_query_matrix_A\n",
      "_orig_mod.blocks.4.sa.heads.0.lora_query_matrix_B\n",
      "_orig_mod.blocks.4.sa.heads.0.lora_value_matrix_A\n",
      "_orig_mod.blocks.4.sa.heads.0.lora_value_matrix_B\n",
      "_orig_mod.blocks.4.sa.heads.0.lora_key_matrix_A\n",
      "_orig_mod.blocks.4.sa.heads.0.lora_key_matrix_B\n",
      "_orig_mod.blocks.4.sa.heads.0.key.weight\n",
      "_orig_mod.blocks.4.sa.heads.0.query.weight\n",
      "_orig_mod.blocks.4.sa.heads.0.value.weight\n",
      "_orig_mod.blocks.4.sa.heads.1.lora_query_matrix_A\n",
      "_orig_mod.blocks.4.sa.heads.1.lora_query_matrix_B\n",
      "_orig_mod.blocks.4.sa.heads.1.lora_value_matrix_A\n",
      "_orig_mod.blocks.4.sa.heads.1.lora_value_matrix_B\n",
      "_orig_mod.blocks.4.sa.heads.1.lora_key_matrix_A\n",
      "_orig_mod.blocks.4.sa.heads.1.lora_key_matrix_B\n",
      "_orig_mod.blocks.4.sa.heads.1.key.weight\n",
      "_orig_mod.blocks.4.sa.heads.1.query.weight\n",
      "_orig_mod.blocks.4.sa.heads.1.value.weight\n",
      "_orig_mod.blocks.4.sa.heads.2.lora_query_matrix_A\n",
      "_orig_mod.blocks.4.sa.heads.2.lora_query_matrix_B\n",
      "_orig_mod.blocks.4.sa.heads.2.lora_value_matrix_A\n",
      "_orig_mod.blocks.4.sa.heads.2.lora_value_matrix_B\n",
      "_orig_mod.blocks.4.sa.heads.2.lora_key_matrix_A\n",
      "_orig_mod.blocks.4.sa.heads.2.lora_key_matrix_B\n",
      "_orig_mod.blocks.4.sa.heads.2.key.weight\n",
      "_orig_mod.blocks.4.sa.heads.2.query.weight\n",
      "_orig_mod.blocks.4.sa.heads.2.value.weight\n",
      "_orig_mod.blocks.4.sa.heads.3.lora_query_matrix_A\n",
      "_orig_mod.blocks.4.sa.heads.3.lora_query_matrix_B\n",
      "_orig_mod.blocks.4.sa.heads.3.lora_value_matrix_A\n",
      "_orig_mod.blocks.4.sa.heads.3.lora_value_matrix_B\n",
      "_orig_mod.blocks.4.sa.heads.3.lora_key_matrix_A\n",
      "_orig_mod.blocks.4.sa.heads.3.lora_key_matrix_B\n",
      "_orig_mod.blocks.4.sa.heads.3.key.weight\n",
      "_orig_mod.blocks.4.sa.heads.3.query.weight\n",
      "_orig_mod.blocks.4.sa.heads.3.value.weight\n",
      "_orig_mod.blocks.4.sa.heads.4.lora_query_matrix_A\n",
      "_orig_mod.blocks.4.sa.heads.4.lora_query_matrix_B\n",
      "_orig_mod.blocks.4.sa.heads.4.lora_value_matrix_A\n",
      "_orig_mod.blocks.4.sa.heads.4.lora_value_matrix_B\n",
      "_orig_mod.blocks.4.sa.heads.4.lora_key_matrix_A\n",
      "_orig_mod.blocks.4.sa.heads.4.lora_key_matrix_B\n",
      "_orig_mod.blocks.4.sa.heads.4.key.weight\n",
      "_orig_mod.blocks.4.sa.heads.4.query.weight\n",
      "_orig_mod.blocks.4.sa.heads.4.value.weight\n",
      "_orig_mod.blocks.4.sa.heads.5.lora_query_matrix_A\n",
      "_orig_mod.blocks.4.sa.heads.5.lora_query_matrix_B\n",
      "_orig_mod.blocks.4.sa.heads.5.lora_value_matrix_A\n",
      "_orig_mod.blocks.4.sa.heads.5.lora_value_matrix_B\n",
      "_orig_mod.blocks.4.sa.heads.5.lora_key_matrix_A\n",
      "_orig_mod.blocks.4.sa.heads.5.lora_key_matrix_B\n",
      "_orig_mod.blocks.4.sa.heads.5.key.weight\n",
      "_orig_mod.blocks.4.sa.heads.5.query.weight\n",
      "_orig_mod.blocks.4.sa.heads.5.value.weight\n",
      "_orig_mod.blocks.4.sa.proj.weight\n",
      "_orig_mod.blocks.4.sa.proj.bias\n",
      "_orig_mod.blocks.4.ffwd.net.0.weight\n",
      "_orig_mod.blocks.4.ffwd.net.2.weight\n",
      "_orig_mod.blocks.4.ln1.weight\n",
      "_orig_mod.blocks.4.ln2.weight\n",
      "_orig_mod.blocks.5.sa.heads.0.lora_query_matrix_A\n",
      "_orig_mod.blocks.5.sa.heads.0.lora_query_matrix_B\n",
      "_orig_mod.blocks.5.sa.heads.0.lora_value_matrix_A\n",
      "_orig_mod.blocks.5.sa.heads.0.lora_value_matrix_B\n",
      "_orig_mod.blocks.5.sa.heads.0.lora_key_matrix_A\n",
      "_orig_mod.blocks.5.sa.heads.0.lora_key_matrix_B\n",
      "_orig_mod.blocks.5.sa.heads.0.key.weight\n",
      "_orig_mod.blocks.5.sa.heads.0.query.weight\n",
      "_orig_mod.blocks.5.sa.heads.0.value.weight\n",
      "_orig_mod.blocks.5.sa.heads.1.lora_query_matrix_A\n",
      "_orig_mod.blocks.5.sa.heads.1.lora_query_matrix_B\n",
      "_orig_mod.blocks.5.sa.heads.1.lora_value_matrix_A\n",
      "_orig_mod.blocks.5.sa.heads.1.lora_value_matrix_B\n",
      "_orig_mod.blocks.5.sa.heads.1.lora_key_matrix_A\n",
      "_orig_mod.blocks.5.sa.heads.1.lora_key_matrix_B\n",
      "_orig_mod.blocks.5.sa.heads.1.key.weight\n",
      "_orig_mod.blocks.5.sa.heads.1.query.weight\n",
      "_orig_mod.blocks.5.sa.heads.1.value.weight\n",
      "_orig_mod.blocks.5.sa.heads.2.lora_query_matrix_A\n",
      "_orig_mod.blocks.5.sa.heads.2.lora_query_matrix_B\n",
      "_orig_mod.blocks.5.sa.heads.2.lora_value_matrix_A\n",
      "_orig_mod.blocks.5.sa.heads.2.lora_value_matrix_B\n",
      "_orig_mod.blocks.5.sa.heads.2.lora_key_matrix_A\n",
      "_orig_mod.blocks.5.sa.heads.2.lora_key_matrix_B\n",
      "_orig_mod.blocks.5.sa.heads.2.key.weight\n",
      "_orig_mod.blocks.5.sa.heads.2.query.weight\n",
      "_orig_mod.blocks.5.sa.heads.2.value.weight\n",
      "_orig_mod.blocks.5.sa.heads.3.lora_query_matrix_A\n",
      "_orig_mod.blocks.5.sa.heads.3.lora_query_matrix_B\n",
      "_orig_mod.blocks.5.sa.heads.3.lora_value_matrix_A\n",
      "_orig_mod.blocks.5.sa.heads.3.lora_value_matrix_B\n",
      "_orig_mod.blocks.5.sa.heads.3.lora_key_matrix_A\n",
      "_orig_mod.blocks.5.sa.heads.3.lora_key_matrix_B\n",
      "_orig_mod.blocks.5.sa.heads.3.key.weight\n",
      "_orig_mod.blocks.5.sa.heads.3.query.weight\n",
      "_orig_mod.blocks.5.sa.heads.3.value.weight\n",
      "_orig_mod.blocks.5.sa.heads.4.lora_query_matrix_A\n",
      "_orig_mod.blocks.5.sa.heads.4.lora_query_matrix_B\n",
      "_orig_mod.blocks.5.sa.heads.4.lora_value_matrix_A\n",
      "_orig_mod.blocks.5.sa.heads.4.lora_value_matrix_B\n",
      "_orig_mod.blocks.5.sa.heads.4.lora_key_matrix_A\n",
      "_orig_mod.blocks.5.sa.heads.4.lora_key_matrix_B\n",
      "_orig_mod.blocks.5.sa.heads.4.key.weight\n",
      "_orig_mod.blocks.5.sa.heads.4.query.weight\n",
      "_orig_mod.blocks.5.sa.heads.4.value.weight\n",
      "_orig_mod.blocks.5.sa.heads.5.lora_query_matrix_A\n",
      "_orig_mod.blocks.5.sa.heads.5.lora_query_matrix_B\n",
      "_orig_mod.blocks.5.sa.heads.5.lora_value_matrix_A\n",
      "_orig_mod.blocks.5.sa.heads.5.lora_value_matrix_B\n",
      "_orig_mod.blocks.5.sa.heads.5.lora_key_matrix_A\n",
      "_orig_mod.blocks.5.sa.heads.5.lora_key_matrix_B\n",
      "_orig_mod.blocks.5.sa.heads.5.key.weight\n",
      "_orig_mod.blocks.5.sa.heads.5.query.weight\n",
      "_orig_mod.blocks.5.sa.heads.5.value.weight\n",
      "_orig_mod.blocks.5.sa.proj.weight\n",
      "_orig_mod.blocks.5.sa.proj.bias\n",
      "_orig_mod.blocks.5.ffwd.net.0.weight\n",
      "_orig_mod.blocks.5.ffwd.net.2.weight\n",
      "_orig_mod.blocks.5.ln1.weight\n",
      "_orig_mod.blocks.5.ln2.weight\n",
      "_orig_mod.ln_f.weight\n",
      "_orig_mod.lm_head.weight\n",
      "_orig_mod.lm_head.bias\n",
      "Compiling the model...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_83411/2454079942.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path,map_location=self.device))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 1\n",
    "eval_iters = 1000\n",
    "import torch\n",
    "model_path = \"./10M_2024-07-21_08-16.pth\"\n",
    "model = LoraGPT(r=10,device=device)\n",
    "print(\"Compiling the model...\\n\")\n",
    "try:\n",
    "    model = torch.compile(model)  # requires PyTorch 2.0\n",
    "except Exception as e:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468720\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(num_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get random batch of data\n",
    "import model_def as m\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - m.block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+m.block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+m.block_size]).astype(np.int64)) for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# Estimate loss on train and val splits\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters) \n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "# Helper function to make large numbers of parameters human-readable\n",
    "def human_readable(num):\n",
    "    magnitude = 0\n",
    "    while abs(num) >= 1000:\n",
    "        magnitude += 1\n",
    "        num /= 1000.0\n",
    "    return '%.0f%s' % (num, ['', 'K', 'M', 'G', 'T', 'P'][magnitude])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "max_iters = 1  # Maximum number of iterations\n",
    "learning_rate = 1e-3 # Initial Learning rate value\n",
    "miles = [int(max_iters * m) for m in [0.7, 0.8, 0.9]]  # Milestones for learning rate decay as fractions of max_iters\n",
    "eval_interval = 100 # Evaluation interval\n",
    "eval_iters = 10000  # Number of iterations for evaluation\n",
    "batch_size=256\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initialize learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=miles, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calc loss...\n"
     ]
    }
   ],
   "source": [
    "# Get current date and hour to get track of experiments\n",
    "now = datetime.datetime.now()\n",
    "date_hour = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "# Train\n",
    "# Start training timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Training loop\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # evaluate the model on the train and val splits and log the losses\n",
    "    if iter % eval_interval == 0:\n",
    "        print(\"calc loss...\")\n",
    "        losses = estimate_loss()\n",
    "        print(f'iter {iter:5d} | train loss {losses[\"train\"]:.4f} | val loss {losses[\"val\"]:.4f}')\n",
    "        \n",
    "    print(\"get batch...\")\n",
    "    # train the model for one iteration\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # forward pass\n",
    "    print('forward pass...')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    print('backward ...')\n",
    "    loss.backward()\n",
    "    print('optimizer step...')\n",
    "    optimizer.step()\n",
    "\n",
    "    # Step the scheduler\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "# End training timer\n",
    "end_time = time.time()\n",
    "print(f'Training time: {(end_time - start_time) / 60}  min')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), f\"{num_parameters}_{date_hour}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.memmap('test.bin', dtype=np.uint16, mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_example(example, model, max_new_tokens=30):\n",
    "\n",
    "    # Split example and determine maximum new tokens allowed\n",
    "    splited_example = example.split(\"# reformulation\")\n",
    "    if not (\"for\" in splited_example[0]):\n",
    "        max_new_tokens = 22\n",
    "    # Encode prompt and prepare for evaluation\n",
    "    encoded_example = torch.tensor(encode(splited_example[0] + \"# reformulation\"), dtype=torch.long).unsqueeze(0).to(device)\n",
    "    prompt_text = splited_example[0] + \"# reformulation\"\n",
    "\n",
    "    result_example = splited_example[-1]\n",
    "\n",
    "    #print(\"result: ==>\",result_example)\n",
    "\n",
    "    # Extract real results from example\n",
    "    #real_results = [float(match.group()) for match in re.finditer(r\"(?<=# )-?\\d+(\\.\\d+)?\", result_example.split('\\n\\n')[0].replace(\"\\n\", \"\"))]\n",
    "\n",
    "    # Generate response from model and extract generated results\n",
    "    response = decode(model.generate(encoded_example, max_new_tokens=max_new_tokens)[0].tolist())\n",
    "    splited_response = response.split(\"# reformulation\")\n",
    "    result_response = splited_response[-1]\n",
    "    #generated_results = [float(match.group()) for match in re.finditer(r\"(?<=# )-?\\d+(\\.\\d+)?\", result_response.split('\\n\\n')[0].replace(\"\\n\", \"\"))]\n",
    "\n",
    "    return prompt_text, result_example, result_response\n",
    "\n",
    "\n",
    "\n",
    "# Write results to file\n",
    "def write_results_to_file(output_file, prompt, real_results, generated_results):\n",
    "    df = pd.DataFrame({\n",
    "        'Prompt': prompt,\n",
    "        'Real_Results': real_results,\n",
    "        'Generated_Results': generated_results\n",
    "    })\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_pair(real, generated_result):\n",
    "    # Determine the length of the shorter and longer strings\n",
    "    min_len = min(len(real), len(generated_result))\n",
    "    max_len = max(len(real), len(generated_result))\n",
    "\n",
    "    # Count the number of matching characters at the same index\n",
    "    match_count = sum(1 for i in range(min_len) if real[i] == generated_result[i])\n",
    "\n",
    "    # Calculate the ratio of matches to the length of the longer string\n",
    "    ratio = match_count / max_len\n",
    "    return ratio\n",
    "\n",
    "# Evaluation Loop\n",
    "\n",
    "# Split examples and initialize lists for results\n",
    "examples = decode(test_data).split(\"\\n\\n\")\n",
    "examples = [example for example in examples if example]\n",
    "\n",
    "# Start evaluation process\n",
    "prompt = []\n",
    "real_results = []\n",
    "generated_results = []\n",
    "\n",
    "# Iterate through examples and evaluate the model on each one\n",
    "for example in tqdm(examples):\n",
    "    prompt_text, real_result, result = evaluate_example(example, model)\n",
    "    prompt.append(prompt_text)\n",
    "    real_results.append(real_result)\n",
    "    generated_results.append(result)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "score=0\n",
    "\n",
    "for real,generated in zip(real_results, generated_results):\n",
    "  score+=evaluate_pair(real,generated)\n",
    "accuracy = score / len(generated_results)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Store accuracy in a file\n",
    "with open(\"accuracy.txt\", 'w') as f:\n",
    "    f.write(f\"Accuracy: {accuracy * 100:.2f}%\\n\")\n",
    "\n",
    "# Store predictions in a CSV file\n",
    "    write_results_to_file(\"predictions.csv\", prompt, real_results, generated_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
